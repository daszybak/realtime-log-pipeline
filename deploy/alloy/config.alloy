discovery.docker "realtime_log_pipeline" {
	host             = "unix:///var/run/docker.sock"
	refresh_interval = "5s"

	filter {
		name   = "label"
		values = ["com.docker.compose.project=realtime-log-pipeline"]
	}
}

loki.process "realtime_log_pipeline" {
	forward_to = [loki.write.default.receiver]

	stage.json {
		expressions = {
			component = "component",
			level     = "level",
			message   = "message",
			service   = "service",
			symbol    = "symbol",
			timestamp = "time",
			trace_id  = "trace_id",
		}
	}

	stage.labels {
		values = {
			component = null,
			level     = null,
			service   = null,
			symbol    = null,
			trace_id  = null,
		}
	}

	stage.timestamp {
		source           = "timestamp"
		format           = "Unix"
		fallback_formats = ["RFC3339", "RFC3339Nano"]
	}

	stage.output {
		source = "message"
	}
}

discovery.relabel "realtime_log_pipeline" {
	targets = []

	rule {
		source_labels = ["__meta_docker_container_name"]
		regex         = "/(.*)"
		target_label  = "container"
	}

	rule {
		source_labels = ["__meta_docker_container_log_stream"]
		target_label  = "stream"
	}

	rule {
		source_labels = ["__meta_docker_container_label_com_docker_compose_service"]
		target_label  = "service"
	}

	rule {
		source_labels = ["__meta_docker_container_label_com_docker_compose_project"]
		target_label  = "compose_project"
	}

	rule {
		source_labels = ["__meta_docker_container_id"]
		regex         = "(.{12}).*"
		target_label  = "container_id"
	}

	rule {
		source_labels = ["__meta_docker_container_label_com_docker_compose_config_files"]
		regex         = ".*(dev).*"
		target_label  = "environment"
		replacement   = "development"
	}

	rule {
		source_labels = ["environment"]
		regex         = "^$"
		target_label  = "environment"
		replacement   = "production"
	}

	rule {
		source_labels = ["__meta_docker_container_label_com_docker_compose_service"]
		regex         = "(api|worker|aggregator|streamer|app)"
		action        = "keep"
	}
}

loki.source.docker "realtime_log_pipeline" {
	host             = "unix:///var/run/docker.sock"
	targets          = discovery.docker.realtime_log_pipeline.targets
	forward_to       = [loki.process.realtime_log_pipeline.receiver]
	relabel_rules    = discovery.relabel.realtime_log_pipeline.rules
	refresh_interval = "5s"
}

discovery.docker "realtime_log_pipeline_infra" {
	host             = "unix:///var/run/docker.sock"
	refresh_interval = "15s"

	filter {
		name   = "label"
		values = ["com.docker.compose.project=realtime-log-pipeline"]
	}
}

loki.process "realtime_log_pipeline_infra" {
	forward_to = [loki.write.default.receiver]

	stage.labels {
		values = {
			service = null,
		}
	}

	stage.output {
		source = "output"
	}
}

discovery.relabel "realtime_log_pipeline_infra" {
	targets = []

	rule {
		source_labels = ["__meta_docker_container_name"]
		regex         = "/(.*)"
		target_label  = "container"
	}

	rule {
		source_labels = ["__meta_docker_container_label_com_docker_compose_service"]
		target_label  = "service"
	}

	rule {
		source_labels = ["__meta_docker_container_label_com_docker_compose_project"]
		target_label  = "compose_project"
	}

	rule {
		source_labels = ["__meta_docker_container_label_com_docker_compose_service"]
		regex         = "(postgres|rabbitmq|prometheus|grafana|loki|jaeger)"
		action        = "keep"
	}
}

loki.source.docker "realtime_log_pipeline_infra" {
	host             = "unix:///var/run/docker.sock"
	targets          = discovery.docker.realtime_log_pipeline_infra.targets
	forward_to       = [loki.process.realtime_log_pipeline_infra.receiver]
	relabel_rules    = discovery.relabel.realtime_log_pipeline_infra.rules
	refresh_interval = "15s"
}

loki.write "default" {
	endpoint {
		url = "http://loki:3100/loki/api/v1/push"
	}
	external_labels = {}
}


// === Prometheus scrapes in Alloy ===

// Prometheus itself (optional, mostly for debugging Alloy)
prometheus.scrape "self" {
  targets         = ["localhost:9090"]
  scrape_interval = "15s"
}

// API service
prometheus.scrape "api" {
  targets         = ["host.docker.internal:8081"]
  scrape_interval = "15s"
  metrics_path    = "/metrics"
}

// Worker service
prometheus.scrape "worker" {
  targets         = ["host.docker.internal:8082"]
  scrape_interval = "15s"
  metrics_path    = "/metrics"
}

// Aggregator service
prometheus.scrape "aggregator" {
  targets         = ["host.docker.internal:8083"]
  scrape_interval = "15s"
  metrics_path    = "/metrics"
}

// Streamer service
prometheus.scrape "streamer" {
  targets         = ["host.docker.internal:8084"]
  scrape_interval = "5s"
  metrics_path    = "/metrics"
}

// RabbitMQ management metrics
prometheus.scrape "rabbitmq" {
  targets         = ["rabbitmq:15692"]
  scrape_interval = "15s"
  metrics_path    = "/metrics"
}

// Optional exporters (commented out)
// prometheus.scrape "postgres" {
//   targets         = ["postgres-exporter:9187"]
//   scrape_interval = "30s"
// }
// prometheus.scrape "cadvisor" {
//   targets         = ["cadvisor:8080"]
//   scrape_interval = "30s"
// }

// === Forward to Prometheus ===
prometheus.remote_write "to_prometheus" {
  endpoint {
    url = "http://prometheus:9090/api/v1/write"
  }
}


// === OpenTelemetry Traces ===

// OTLP receiver (services send traces to Alloy on gRPC 4317 and HTTP 4318)
otlp.receiver "default" {
  grpc {
    endpoint = "0.0.0.0:4317"
  }
  http {
    endpoint = "0.0.0.0:4318"
  }
}

// Export traces to Jaeger all-in-one (already in docker-compose)
otelcol.exporter.jaeger "default" {
  endpoint {
    url = "http://jaeger:14268/api/traces"
  }
}

// Pipeline: OTLP -> Jaeger
otelcol.processor.batch "default" {
  timeout  = "10s"
  send_batch_size = 8192
}

otelcol.service "traces" {
  pipelines {
    traces = [
      otlp.receiver.default,
      otelcol.processor.batch.default,
      otelcol.exporter.jaeger.default,
    ]
  }
}
